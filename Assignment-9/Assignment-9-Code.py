# -*- coding: utf-8 -*-
"""Copy of Probability_2022_student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F4u-g2Z-EHnhnkn6uOV2vtnUEXdwbgNz

# Attention !!!
- Please write or take a screenshot of all answers in the pdf file. You won't be graded if there is no pdf file in the submission.
- Only TODO 1, 2, 3, 4, 5, 6, 10, 11 will be graded.
- **Extra credit:** 1% of total grade for Com Eng Math 2 for TODO 7, 8, 9 (1/3 each.)

# Sampling

Sampling is a process that is very important for writing simulations. In this section, you will try to sample from some common distributions.

TODO#1: Write functions that samples from the following distribution
1. $\mathcal{N}(0,1)$
2. $Bernoulli(0.3)$
3. $B(10, 0.3)$
4. $Multinomial(n=10, p=[0.3,0.2,0.5])$
5. $U(0,1)$
<!-- 6. $T(0,1)$; $T(a,b)$ is defined as a function with a shape of a triangle that pass through point $(a,0)$, $(b,0)$, and $(b, K):\frac{(b-a)K}{2}=1$. -->
6. $T(0,1)$; $T(a,b)$ is defined as a function with a shape of a triangle that pass through point $(a,0)$, $(b,0)$, and $(\frac{a+b}{2}, K):\frac{(b-a)K}{2}=1$.

Capture screenshot of the histogram for each of the distribution and paste them on the pdf file. The example is shown below.

Hint: see scipy.stats for common distributions.
[plt.hist](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) should be helpful for plotting histograms
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, bernoulli, binom, multinomial, uniform, expon, triang


def sample_normal(sample_size=1000000, mu=0, std=1):
  # TODO#1.1: #
  ##
  return norm.rvs(loc = mu, scale = std, size = sample_size)


def sample_bernoulli(sample_size=1000000, p=0.3):
  # TODO#1.2:
  ##
  return bernoulli.rvs(p = p, size = sample_size)


def sample_binomial(sample_size=1000000, n=10, p=0.3):
  # TODO#1.3:
  ##
  return binom.rvs(n = n, p = p, size = sample_size)


def sample_multinomial(sample_size=1000000, n=10, p=[0.3, 0.2, 0.5]):
  # TODO#1.4:
  ##
  return multinomial.rvs(n = n, p = p, size = sample_size)


def sample_uniform(sample_size=1000000, from_x=0, to_x=1):
  # TODO#1.5:
  ##
  return uniform.rvs(from_x, to_x, size = sample_size)


def sample_triangle(sample_size=1000000, a=0, b=1):
  # TODO#1.6:
  ##
  return triang.rvs(b/2, loc = a, size = sample_size)

s = sample_normal()
count, bins, ignored = plt.hist(s, 100, density=False)

plt.title('Normal distribution')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s = sample_bernoulli()
count, bins, ignored = plt.hist(s, 100, density=False)

plt.title('Bernoulli distribution')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s = sample_binomial()
count, bins, ignored = plt.hist(s, 100, density=False)

plt.title('Binomial distribution')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s = sample_multinomial()
count, bins, ignored = plt.hist(s, 20, density=False)

plt.title('Multinomial distribution')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s = sample_uniform()
count, bins, ignored = plt.hist(s, 100, density=False)

plt.title('Uniform distribution')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s = sample_triangle()
count, bins, ignored = plt.hist(s, 100, density=False)

plt.title('Triangle distribution')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

"""# Law of large number

### Law of large number

**TODO#2:** Using a sampling function from TODO#1.1, Plot the graph that shows the relation between an empirical mean and sampling size from 1 up to 10000.
What does the graph imply about the difference between the empirical mean and the theoritical mean?
"""

sampling_size = np.arange(0, 10000)
empirical_mean = [np.mean(sample_normal(s, 0, 1)) for s in sampling_size]
theoritical_mean = [0 for s in sampling_size]
plt.plot(sampling_size, empirical_mean, label = "Empirical Mean")
plt.plot(sampling_size, theoritical_mean, color = 'r', label = "Theoritical Mean")
plt.title("Empirical Mean VS Theoritical Mean")
plt.ylabel("Mean")
plt.xlabel("Sampling Size")
plt.legend()

"""### Law of large number for histogram

The histogram is used to approximate the PDF of an unknown distribution.
The bin in the histogram represents the frequency of the event happening inside the bin range.

**TODO#3:** Given a fix bin number of 40. Plot the histogram of the data sampling from the function, `sample_normal(n, 0, 1)`, for different sizes of sample: 500, 1k, 5k and 10k.
Compare and explain the relation between the approximation given by the histogram and the true PDF for each of the sample size.
"""

bin_num = 40
s500 = sample_normal(sample_size = 500, mu=0, std=1)
count, bins, ignored = plt.hist(s500, bin_num, density=True)

x = np.linspace(norm.ppf(0.01, loc=0, scale=1),norm.ppf(0.99, loc=0, scale=1))
plt.plot(x, norm.pdf(x, loc=0, scale=1))

plt.title('Normal distribution with 500 samples')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s1k = sample_normal(sample_size = 1000)
count, bins, ignored = plt.hist(s1k, bin_num, density=True)

x = np.linspace(norm.ppf(0.01, loc=0, scale=1),norm.ppf(0.99, loc=0, scale=1))
plt.plot(x, norm.pdf(x, loc=0, scale=1))

plt.title('Normal distribution with 1000 samples')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s5k = sample_normal(sample_size = 5000)
count, bins, ignored = plt.hist(s5k, bin_num, density=True)

x = np.linspace(norm.ppf(0.01, loc=0, scale=1),norm.ppf(0.99, loc=0, scale=1))
plt.plot(bins, norm.pdf(bins, loc=0, scale=1))

plt.title('Normal distribution with 5000 samples')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

s10k = sample_normal(sample_size = 10000)
count, bins, ignored = plt.hist(s10k, bin_num, density=True)


x = np.linspace(norm.ppf(0.01, loc=0, scale=1),norm.ppf(0.99, loc=0, scale=1), 100)
plt.plot(x, norm.pdf(x, loc=0, scale=1))


plt.title('Normal distribution with 10000 samples')
plt.ylabel('Density')
plt.xlabel('X')
plt.show()

"""## Central limit theorem

In this part we will use the Central Limit Theorem to approximate the true probabity of getting more than 40 heads when an unfair coin, with the probability 0.3 of being head, is tossed 100 times.


**TODO#4:** Simulate multiple coin tosses to construct a histrogram from the outcomes. Plot the histogram. Hint: x-axis should represents the number of heads when the coin is tossed 100 times. Does this histogram looks like a normal distribution?
"""

s = sample_binomial(sample_size=100000, n=100, p=0.3)
count, bins, ignored = plt.hist(s, 100, density=False)

plt.title('Binomial distribution')
plt.ylabel('Density')
plt.xlabel('Number of Heads')
plt.show()

"""**TODO#5:** Use CLT to find the probability of getting more than 40 heads.

**TODO#6:** Compare and find the difference between CLT's approximation and the actual probability using the binomial distribution.
"""

# TODO#5
n = 100
p = 0.3

z = (40 - binom.mean(n, p, loc=0))/binom.std(n, p, loc=0)
probability = 1-norm.cdf(z)
print(probability)

# TODO#6
count = 0
for c in s:
  if c >= 41:
    count += 1
print(count/len(s))
diff = abs(probability-count/len(s))
print(diff)

"""# Algebra of Random Variables

Given an independent random variable $X$ and $Y$, such that $X \sim F$ and $Y \sim U(3,5)$. The summation of those two is written as $Z = X + Y$ and the PDF of $F$ is defined below.
$$
F(X) =
\begin{cases}
0.1, & -2<=X<=0\\
0.4, & 0<X<=2 \\
\end{cases}
$$
**TODO#7:** Find $P( 3 < Z < 5 )$.
"""

import scipy.integrate as integrate

delta = 0.001

tx = np.arange(-3, 3, delta)
X = 0.1*np.heaviside(tx+2, 1) + 0.3*np.heaviside(tx, 1) - 0.4*np.heaviside(tx-2, 1)

ty = np.arange(2, 6, delta)
Y = (np.heaviside(ty-3, 1) - np.heaviside(ty-5, 1))/(5-3)

# have to find p(z) = p(x+y) = p(x)*p(y)
tz = np.arange(-1, 9-delta, delta)
Z = np.convolve(X, Y)*delta

result_start = 0
result_start_index = (result_start+1)*1000
result_stop = 8
result_stop_index = (result_stop+1)*1000
result = round(np.sum(Z[result_start_index:result_stop_index+1])*delta, 3)

partial_start = 3
partial_start_index = (partial_start+1)*1000
partial_stop = 5
partial_stop_index = (partial_stop+1)*1000
partial = round(np.sum(Z[partial_start_index:partial_stop_index+1])*delta, 3)

possibility = partial/result
print(possibility)

"""# Correlation

The correlation captures the linear relationshi between two sets of random variables. The higher magnitude of the correlation indicates a stronger relationship.


**TODO#8:** Find the correlation of $X$ and $Y = X + A$, given that $X \sim U(-1,1)$ and
1. $A = 10$
2. $A \sim U(-1,1)$
3. $A \sim U(-10,10)$
4. $A \sim U(-100,100)$


**TODO#9:** From the results in TODO#8, answer following questions
1. Does the correlation decrease as we increase the randomness of A ?
2. Explain the result when we change from $A \sim U(-10,10)$ to $A \sim U(9090,10010)$. Hint: Compare the result with $A$ and $A + 10000: A \sim U(-10,10) $
"""

# TODO#8
X = sample_uniform(sample_size=100000, from_x=-1, to_x=1)

# 8.1
A1 = 10
Y1 = X + A1
print("correlation of  X  and  Y1 : ", np.corrcoef(X, Y1)[0][1])

# 8.2
A2 = sample_uniform(sample_size=100000, from_x=-1, to_x=1)
Y2 = X + A2
print("correlation of  X  and  Y2 : ", np.corrcoef(X, Y2)[0][1])

# 8.3
A3 = sample_uniform(sample_size=100000, from_x=-10, to_x=10)
Y3 = X + A3
print("correlation of  X  and  Y3 : ", np.corrcoef(X, Y3)[0][1])

# 8.4
A4 = sample_uniform(sample_size=100000, from_x=-100, to_x=100)
Y4 = X + A4
print("correlation of  X  and  Y4 : ", np.corrcoef(X, Y4)[0][1])

# TODO#9
A5 = A3+10000
Y5 = X + A5
print("correlation of  X  and  Y3 : ", np.corrcoef(X, Y3)[0][1])
print("correlation of  X  and  Y5 : ", np.corrcoef(X, Y5)[0][1])

"""# Hamtaro and his cloud storage empire.

After the success in the manufacturing business. Hamtaro wants to expand his business into a new sector.
Since cloud computing is currently booming, he decides to enter into the cloud storage business.

The storage disk that Hamtaro uses can operate only in the temperature of $[0,30]$ degree Celcius. The disk has the prabability of a read failure $P(Fail|t) = \frac{0.97}{2250}(t-15)^2+0.001$ where $t$ is the operating temperature.
<!-- Hamtaro's disks has a special architecture that can be  simultaneously read by infinite requests at the same time. However, a failure of one request will cause all of its parallels to fail. -->

Since Hamtoro doesn't want any failures in his service, he decides to buy a super luxury air-conditioning system to control the temperature in his data warehouse. Even if the air conditioner is extremely expensive, the room temperature is still not stable. When Hamtaro tries to set the tempurature to $\mu$, the actual temperature is random and can be modeled by $t\sim U(\mu-1,\mu+1)$.

**TODO#10:** Answer the following questions.
1. What is the temperature that Hamtaro should set the air conditioner to? Justify your answer.
2. What is the probability of failure at the temperature used in part 1?
<!-- 3. If Hamtaro want to handle 10k concurrent requests, what is the minimum disks should Hamtoro has to make 99.99% of disk availability and how should he split the workloads? Hamtaro connects the all the disks in parallel. The read request will fail if all disks fail to be read at the same time. -->
3. What is the minimum number of disks that Hamtoro has to use to make sure that the probability of having more than 1 failure in 10k requests is less than 0.01\%? Hamtaro connects the all the disks in parallel. The read request will fail if all disks fail to at the same time.
4. **Extra** The temperature is now modeled by $t\sim \mathcal{N}(\mu,9)$ instead of $t\sim U(\mu-1,\mu+1)$. Repeat question 1-3.

**Hint:** `scipy.integrate.quad` can help you do integration.
"""

# TODO#10
# answers are in pdf file

"""# Moontaro
![picture](https://i.redd.it/mcveltqx2j071.png)

Recently, cryptocurrency investment has become extremely popular due to its extraordinarily high rates of return. Though many people consider it a risky investment, Hamtaro does not want to miss this opportunity and start gathering information about these coins. His research suggests that four coins, namely $a$, $b$, $c$, and $d$, have a promising future to go to the moon.


Hamtaro wants to run simulations to validate his chances. As the value of the coins is non-deterministic, he models it sequentially based on their historical values (a.k.a. autoregressive model). The price of coin $i$ at day $t$ is formulated as

$p_{i,t} = p_{i,t-1} \times r_{i,t}$, where $i \in \{a, b, c, d\}$, and  $p_{i, 0} = 10$.

The rates $r_{i,t}$, are drawn from a multivariant guassian distribution $\mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$, where $\mu = [1.003, 1.002, 1.004, 1.004]^T$ and $\mathbf{\Sigma}$ as given below:

$\mathbf{\Sigma}$ | a | b | c | d
--- | --- | --- | --- |---
a |  10 x 10$^{-3}$ | 0 | 4 x 10$^{-3}$ | 5 x 10$^{-3}$
b |  0  | 3 x 10$^{-3}$ | 0 | 0
c |  4 x 10$^{-3}$  | 0 | 12 x 10$^{-3}$| 2 x 10$^{-3}$
d |  5 x 10$^{-3}$  | 0 | 2 x 10$^{-3}$ | 15 x 10$^{-3}$


<!-- 1. Are $p_{a,t}$ and $p_{b,t}$ independent ? Why ?
2. Are $p_{a,t}$ and $p_{c,t}$ independent ? Why ?
3. Are $p_{a,t}$ and $p_{d,t}$ independent ? Why ? -->

**TODO11:**
1. Which pairs of coins are independent? Why?
2. Given the following definitions:
  - <b>Return</b> :  a coin price at day $T$ minus the price at day 0, i.e., the return of coin $i$ at day $T = p_{i,T} - p_{i, 0}$.
  - <b>Expected return</b> : the average return from 10000 distinct simulated end prices.
  
  Simulate the expected return for each coin if Hamtaro wants to sell his coins 30 and 180 days after buying $(T \in \{30, 180\})$.
  hint: you should write reusable functions to make your life easier.
3. Which coin has the highest probability of having profit (end price is higher than start price)? Compare the variance of the return with other coins.
4. How can the expected return be positive while having around 50\% chance of profitability?

After simulating the price of individual coins, Hamtaro now proposes seven investment strategies (portfolio) to maximize the profit. The detail of each strategy is shown in the table below.  

Strategy | Buy $a$ | Buy $b$ | Buy $c$ | Buy $d$ | Expected\[return\] | Variance\[return\] | Probability of having profit
---| --- |--- |--- | ---| --- | --- | ---
1  | 100% | 0%   | 0%  |    0%|  |  |
2  | 0%   | 100% | 0%  |    0%|  |  |
3  | 0%   | 0%   | 100%|    0%|  |  |
4  | 0%   | 0%   |   0%|  100%|  |  |
5  | 50%  | 50%  | 0%  |    0%|  |  |
6  | 50%  | 0%   | 50% |    0%|  |  |
7  | 50%  | 0%   |  0% |   50%|  |  |

5. Fill the empty values in the table (both $T = 30, 180$).
6. Which strategy yields the highest return?
7. Which strategy is the safest one?
8. Compare the variances between the stategy 6 and 7. What happens, and why is this the case? **Hint:** Consider cov($r_a$, $r_c$) and cov($r_a$, $r_d$).
9. From the problems above, come up with a general practice for good investment? Please also state your reasoning. You can include additional simulations to support the argument.
"""

from scipy.stats import multivariate_normal
miu = [1.003,1.002,1.004,1.004]
cov_matrix = [[10*10**(-3),   0,          4*10**(-3),   5*10**(-3)],
              [0,             3*10**(-3), 0,            0],
              [4*10**(-3),    0,          12*10**(-3),  2*10**(-3)],
              [5*10**(-3),    0,          2*10**(-3),   15*10**(-3)]]
day0 = [10, 10, 10, 10]

# TODO#11.2
def expected_return(n):
  sampling_size = 10000
  expected_value = [0, 0, 0, 0]
  for i in range(sampling_size): # simulate 10000 returns
    return_value = day0
    rits = multivariate_normal.rvs(mean=miu, cov=cov_matrix, size = n)
    rits_product = np.prod(rits, axis=0)
    return_value *= rits_product
    expected_value += (return_value-day0)/sampling_size
  return expected_value

result_30days = expected_return(30)
print("Expected return of coin a after 30 days : ", result_30days[0])
print("Expected return of coin b after 30 days : ", result_30days[1])
print("Expected return of coin c after 30 days : ", result_30days[2])
print("Expected return of coin d after 30 days : ", result_30days[3])
print()
result_180days = expected_return(180)
print("Expected return of coin a after 180 days : ", result_180days[0])
print("Expected return of coin b after 180 days : ", result_180days[1])
print("Expected return of coin c after 180 days : ", result_180days[2])
print("Expected return of coin d after 180 days : ", result_180days[3])

# TODO#11.3
def profit_probability(n):
  sampling_size = 10000
  count_profit = np.array([0, 0, 0, 0])
  for i in range(sampling_size): # simulate 10000 returns
    rits = multivariate_normal.rvs(mean=miu, cov=cov_matrix, size = n)
    rits_product = np.prod(rits, axis=0)
    for j in range(4):
      if rits_product[j] > 1:
        count_profit[j] += 1
  return count_profit/sampling_size

days = 100
profit_prob = profit_probability(days)
print("a :", profit_prob[0])
print("b :", profit_prob[1])
print("c :", profit_prob[2])
print("d :", profit_prob[3])

# TODO#11.3 (cont.)
def variance(n):
  sampling_size = 10000
  expected_value = [0, 0, 0, 0]
  sqrt_expected_value = [0, 0, 0, 0]
  for i in range(sampling_size): # simulate 10000 returns
    return_value = day0
    rits = multivariate_normal.rvs(mean=miu, cov=cov_matrix, size = n)
    rits_product = np.prod(rits, axis=0)
    return_value *= rits_product
    expected_value += (return_value-day0)/sampling_size
    sqrt_expected_value += (return_value-day0)**2/sampling_size
  return sqrt_expected_value-expected_value**2

var = variance(days)
print("a :", var[0])
print("b :", var[1])
print("c :", var[2])
print("d :", var[3])

# TODO#11.4
# answer is in pdf file
pass

# TODO#11.5
ExpectedReturn30 = expected_return(30)
ExpectedReturn180 = expected_return(180)
VarianceReturn30 = variance(30)
VarianceReturn180 = variance(180)
ProbabilityProfit30 = profit_probability(30)
ProbabilityProfit180 = profit_probability(180)

strategy = [[1, 0, 0, 0],
            [0, 1, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0.5, 0.5, 0, 0],
            [0.5, 0, 0.5, 0],
            [0.5, 0, 0, 0.5],]

for i in range(len(strategy)):
  print("STRATEGY #", i+1)
  print("T = 30 :")
  print("\tExpected[return] :", round(np.sum(ExpectedReturn30*strategy[i]), 3))
  print("\tVariance[return] :", round(np.sum(VarianceReturn30*strategy[i]), 3))
  print("\tProbability of having profit :", round(np.sum(ProbabilityProfit30*strategy[i]), 3))
  print("T = 180 :")
  print("\tExpected[return] :", round(np.sum(ExpectedReturn180*strategy[i]), 3))
  print("\tVariance[return] :", round(np.sum(VarianceReturn180*strategy[i]), 3))
  print("\tProbability of having profit :", round(np.sum(ProbabilityProfit180*strategy[i]), 3))
  print()

# TODO#11.6
# answer is in pdf file
pass

# TODO#11.7
# answer is in pdf file
pass

# TODO#11.8
def cov(n):
  sampling_size = 10000
  r = [[], [], [], []]
  for i in range(sampling_size):
    rits = multivariate_normal.rvs(mean=miu, cov=cov_matrix, size = n)
    rits_product = np.prod(rits, axis=0)
    for j in range(4):
      r[j] += [rits_product[j]]
  return r

coin_name = ['a', 'b', 'c', 'd']
coin30 = cov(30)
coin180 = cov(180)

print("T = 30")
for i in range(4):
  for j in range(i+1, 4):
    print("Covariance of", coin_name[i], "and", coin_name[j],
          ":", round(np.cov(coin30[i], coin30[j])[0][1], 6))
print("\nT = 180")
for i in range(4):
  for j in range(i+1, 4):
    print("Covariance of", coin_name[i], "and", coin_name[j],
          ":", round(np.cov(coin180[i], coin180[j])[0][1], 6))

# TODO#11.9
# answer is in pdf file
pass